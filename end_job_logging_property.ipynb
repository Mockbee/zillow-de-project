{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ac695e7-be67-42d5-8873-48e3b191dbf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "start_task = \"job_logging_start\"\n",
    "all_tasks = [\n",
    "    \"FetchingPropertyAPI\",\n",
    "    \"Bronze_Layer_Property\",\n",
    "    \"check_for_property_data\",\n",
    "    \"silver_layer_property\",\n",
    "    \"move_to_archive\"\n",
    "]\n",
    "debug_job_id = \"unknown_job_id\"\n",
    "\n",
    "# STEP 1: Get job_id from start task\n",
    "try:\n",
    "    job_id = dbutils.jobs.taskValues.get(\n",
    "        taskKey=start_task,\n",
    "        key=\"job_id\",\n",
    "        debugValue=debug_job_id\n",
    "    )\n",
    "except Exception:\n",
    "    job_id = debug_job_id\n",
    "\n",
    "# STEP 2: Evaluate job status\n",
    "job_status = \"Succeeded\"\n",
    "skipped_detected = False\n",
    "\n",
    "for task in all_tasks:\n",
    "    try:\n",
    "        error = dbutils.jobs.taskValues.get(taskKey=task, key=\"error\", debugValue=None)\n",
    "        if error:  # Explicit failure\n",
    "            job_status = \"Failed\"\n",
    "            break\n",
    "    except Exception:\n",
    "        skipped_detected = True  # Could not access task's status, likely skipped\n",
    "\n",
    "# Finalize status\n",
    "if job_status != \"Failed\":\n",
    "    job_status = \"Skipped\" if skipped_detected else \"Succeeded\"\n",
    "\n",
    "# STEP 3: Update job_control table\n",
    "update_query = f\"\"\"\n",
    "    UPDATE zillow.propertyextended.job_control\n",
    "    SET\n",
    "        job_status = '{job_status}',\n",
    "        end_time = current_timestamp()\n",
    "    WHERE\n",
    "        job_id = '{job_id}'\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    spark.sql(update_query)\n",
    "    print(f\"Job status updated for job_id = {job_id} with status = {job_status}\")\n",
    "except AnalysisException as sql_ex:\n",
    "    print(f\"SQL error while updating job_control: {sql_ex}\")\n",
    "except Exception as ex:\n",
    "    print(f\"Unexpected error while updating job_control: {ex}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41b6f8f1-feb3-4f7e-b06b-b0d2de98cc04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from zillow.propertyextended.job_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6b6f87a-6cf5-4127-b622-defc1edc9f7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7331595084422060,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "end_job_logging_property",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
