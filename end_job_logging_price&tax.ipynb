{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7595c248-d6d4-4689-8d73-7484e4905d40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "start_task = \"job_logging_start\"\n",
    "all_tasks = [\n",
    "    \"FetchingTaxandPriceData\",\n",
    "    \"Bronze_Layer_Price\",\n",
    "    \"Bronze_Layer_Tax\",\n",
    "    \"check_for_price_data\",\n",
    "    \"silver_layer_price\",\n",
    "    \"check_for_tax_data\",\n",
    "    \"silver_layer_tax\",\n",
    "    \"move_to_archive\"\n",
    "]\n",
    "debug_job_id = \"unknown_job_id\"\n",
    "\n",
    "# STEP 1: Get job_id from start task\n",
    "try:\n",
    "    job_id = dbutils.jobs.taskValues.get(\n",
    "        taskKey=start_task,\n",
    "        key=\"job_id\",\n",
    "        debugValue=debug_job_id\n",
    "    )\n",
    "except Exception:\n",
    "    job_id = debug_job_id\n",
    "\n",
    "# STEP 2: Determine overall job status\n",
    "job_status = \"Succeeded\"\n",
    "skipped_detected = False\n",
    "\n",
    "for task in all_tasks:\n",
    "    try:\n",
    "        error = dbutils.jobs.taskValues.get(taskKey=task, key=\"error\", debugValue=None)\n",
    "        if error is not None:\n",
    "            job_status = \"Failed\"\n",
    "            break\n",
    "    except Exception as e:\n",
    "        # If task was skipped or its output is not accessible\n",
    "        skipped_detected = True\n",
    "\n",
    "# Finalize job status\n",
    "if job_status != \"Failed\":\n",
    "    job_status = \"Skipped\" if skipped_detected else \"Succeeded\"\n",
    "\n",
    "# STEP 3: Update job_control table\n",
    "update_query = f\"\"\"\n",
    "    UPDATE zillow.taxhistory.job_control\n",
    "    SET\n",
    "        job_status = '{job_status}',\n",
    "        end_time = current_timestamp()\n",
    "    WHERE\n",
    "        job_id = '{job_id}'\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    spark.sql(update_query)\n",
    "    print(f\"Job status updated for job_id = {job_id} with status = {job_status}\")\n",
    "except AnalysisException as sql_ex:\n",
    "    print(f\"SQL error while updating job_control: {sql_ex}\")\n",
    "except Exception as ex:\n",
    "    print(f\"Unexpected error while updating job_control: {ex}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcd8c4a8-c68c-415e-bb6a-7036711da66e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from zillow.taxhistory.job_control;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7928624456870446,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "end_job_logging_price&tax",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
