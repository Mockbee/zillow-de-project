{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ac695e7-be67-42d5-8873-48e3b191dbf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "start_task = \"job_logging_start\"\n",
    "#  All actual task keys from your pipeline (excluding the logging tasks)\n",
    "all_tasks = [\n",
    "    \"FetchingPropertyAPI\",\n",
    "    \"Bronze_Layer_Property\",\n",
    "    \"check_for_property_data\",\n",
    "    \"silver_layer_property\",\n",
    "    \"move_to_archive\"\n",
    "]\n",
    "debug_job_id = \"unknown_job_id\"\n",
    "\n",
    "# --------------------------\n",
    "# STEP 1: Get job_id from start task\n",
    "# --------------------------\n",
    "try:\n",
    "    job_id = dbutils.jobs.taskValues.get(\n",
    "        taskKey=start_task,\n",
    "        key=\"job_id\",\n",
    "        debugValue=debug_job_id\n",
    "    )\n",
    "except Exception as e:\n",
    "    job_id = debug_job_id\n",
    "\n",
    "# --------------------------\n",
    "# STEP 2: Collect error messages from each task\n",
    "# --------------------------\n",
    "errors = []\n",
    "\n",
    "for task in all_tasks:\n",
    "    try:\n",
    "        error = dbutils.jobs.taskValues.get(taskKey=task, key=\"error\", debugValue=None)\n",
    "        if error:\n",
    "            errors.append(f\"{task}: {error}\")\n",
    "    except Exception as e:\n",
    "        errors.append(f\"{task}: Unable to retrieve error - {str(e)}\")\n",
    "\n",
    "# --------------------------\n",
    "# STEP 3: Determine status and format error message\n",
    "# --------------------------\n",
    "if errors:\n",
    "    job_status = \"Failed\"\n",
    "    error_message = \"; \".join(errors)\n",
    "else:\n",
    "    job_status = \"Succeeded\"\n",
    "    error_message = None\n",
    "\n",
    "# Escape error message for SQL safety\n",
    "if error_message:\n",
    "    escaped_msg = error_message.replace(\"'\", \"''\")\n",
    "    error_msg_sql = f\"'{escaped_msg}'\"\n",
    "else:\n",
    "    error_msg_sql = \"NULL\"\n",
    "\n",
    "# --------------------------\n",
    "# STEP 4: Update job_control table\n",
    "# --------------------------\n",
    "update_query = f\"\"\"\n",
    "    UPDATE zillow.propertyextended.job_control\n",
    "    SET\n",
    "        job_status = '{job_status}',\n",
    "        end_time = current_timestamp(),\n",
    "        error_message = {error_msg_sql}\n",
    "    WHERE\n",
    "        job_id = '{job_id}'\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    spark.sql(update_query)\n",
    "    print(f\"Job status updated for job_id = {job_id} with status = {job_status}\")\n",
    "    if error_message:\n",
    "        print(\"Error details:\", error_message)\n",
    "except AnalysisException as sql_ex:\n",
    "    print(f\"SQL error while updating job_control: {sql_ex}\")\n",
    "except Exception as ex:\n",
    "    print(f\"Unexpected error while updating job_control: {ex}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5417031813664481,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "end_job_logging",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
